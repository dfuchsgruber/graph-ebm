{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GEBM\n",
    "This an example of how GEBM could be used outside the framework of our project as a standalone module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from graph_uq.gebm import GraphEBMWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gebm = GraphEBMWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the GEBM to the logits and embeddings of your GNN\n",
    "We fit GEBM to the logits and embeddings of your model. The embeddings are used to fit a latent space Gaussian regularizer. The logits are needed to compute a normalizer that scales the logit-based energy term and the regularizer to the same scale.\n",
    "\n",
    "Note that the logits and embeddings should be computed in the presence of network effects, i.e. your normal GNN outputs. Additionally, you should provide a mask for the nodes you have labels for, i.e. training nodes. The regularizer will fit class-conditional Gaussians based on these labels.\n",
    "\n",
    "Your GNN model that predicts some logits of shape [num_nodes, num_classes] and embeddings of shape [num_nodes, emb_dim]. Here, we have dummy code for generating the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn(\n",
    "    200, 5\n",
    ")  # logits outputted by your GNN model in the presence of edges\n",
    "embeddings = torch.randn(\n",
    "    200, 32\n",
    ")  # embeddings outputted by your GNN model in the presence of edges\n",
    "y = torch.randint(0, 5, (200,))  # true class labels\n",
    "edge_index = torch.randint(0, 200, (2, 1000))  # edge index tensor\n",
    "edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)  # make graph undirected\n",
    "mask_train = (\n",
    "    torch.rand(200) < 0.5\n",
    ")  # train mask, the model will fit its regularizers to nodes of this mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gebm.fit(logits, embeddings, edge_index, y, mask_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the GEBM on your model outputs\n",
    "\n",
    "Now you can evaluate GEBM on outputs of your GNN, e.g. on data with a distribution shift. This time, logits and embeddings should be computed **without using network effects**, e.g. by setting the adjacency matrix to the identity or passing an empty edge index tensor to your model. The diffusion of GEBM itself, however, uses the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_eval_no_network = torch.randn(\n",
    "    200, 5\n",
    ")  # logits outputted by your GNN model in the absence of edges\n",
    "embeddings_eval_no_network = torch.randn(\n",
    "    200, 32\n",
    ")  # embeddings outputted by your GNN model in the absence of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainty = gebm.get_uncertainty(\n",
    "    logits_unpropagated=logits_eval_no_network,\n",
    "    embeddings_unpropagated=embeddings_eval_no_network,\n",
    "    edge_index=edge_index,\n",
    ")\n",
    "uncertainty.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GEBM within our framework\n",
    "\n",
    "You can also use GEBM within our framework, i.e. by creating a model that inherits from `BaseModel` and using a dataset that inherits from `Data`, or simply using models and datasets from our codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_uq.config.data import default_data_config\n",
    "from graph_uq.config.model import default_model_config\n",
    "from graph_uq.config.trainer import default_trainer_config\n",
    "\n",
    "from graph_uq.model.build import get_model\n",
    "from graph_uq.data.build import apply_distribution_shift_and_split, get_base_data\n",
    "from graph_uq.training import train_model\n",
    "from graph_uq.logging.logger import Logger\n",
    "from graph_uq.evaluation.uncertainty import binary_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:00:29] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Leaving out classes: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">]</span>     <a href=\"file:///nfs/homedirs/fuchsgru/graph_uq/graph_uq/graph_uq/data/distribution_shift.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">distribution_shift.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///nfs/homedirs/fuchsgru/graph_uq/graph_uq/graph_uq/data/distribution_shift.py#146\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:00:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Leaving out classes: \u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m     \u001b]8;id=892523;file:///nfs/homedirs/fuchsgru/graph_uq/graph_uq/graph_uq/data/distribution_shift.py\u001b\\\u001b[2mdistribution_shift.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=133948;file:///nfs/homedirs/fuchsgru/graph_uq/graph_uq/graph_uq/data/distribution_shift.py#146\u001b\\\u001b[2m146\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CoraML dataset, leave out classes setting\n",
    "\n",
    "data_config = default_data_config.copy()\n",
    "data_config[\"name\"] = \"cora_ml\"\n",
    "data_config[\"categorical_features\"] = True\n",
    "data_config[\"distribution_shift\"][\"type_\"] = \"leave_out_classes\"\n",
    "data_config[\"distribution_shift\"][\"leave_out_classes_type\"] = \"last\"\n",
    "data_config[\"distribution_shift\"][\"num_left_out_classes\"] = 3\n",
    "\n",
    "dataset = apply_distribution_shift_and_split(get_base_data(data_config), data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = default_model_config.copy()\n",
    "model_config[\"name\"] = \"gcn\"\n",
    "model_config[\"type_\"] = \"gcn\"\n",
    "model_config[\"hidden_dims\"] = [64]\n",
    "\n",
    "model = get_model(default_model_config, dataset.data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = default_trainer_config.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train our model. This takes usually 500-1500 epochs on CoraML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 455/10000 [00:20<07:18, 21.77it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = train_model(trainer_config, dataset.data_train, model, Logger())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model.reset_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate GEBM using the wrapper. It provides utility to interact very nicely with dataset and model classes of our framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gebm = GraphEBMWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu().eval()\n",
    "model.reset_cache()\n",
    "gebm.fit_from_model(\n",
    "    dataset.data_train.cpu(),\n",
    "    model.cpu(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_cache()\n",
    "uncertainty = gebm.get_uncertainty_from_model(\n",
    "    dataset.data_shifted[\"loc\"],\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{auc_roc: 0.8890266654532217, auc_pr: 0.8337349197105466}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = dataset.data_shifted[\"loc\"].get_mask(\"test\")\n",
    "binary_classification(\n",
    "    uncertainty[mask], dataset.data_shifted[\"loc\"].get_distribution_mask(\"ood\")[mask]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_uq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
